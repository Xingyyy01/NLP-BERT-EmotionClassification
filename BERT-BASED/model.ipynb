{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½½å…¥åŸå§‹æ•°æ®\n",
    "def load_data(base_path):\n",
    "    paths = os.listdir(base_path)\n",
    "    result = []\n",
    "    for path in paths:\n",
    "        with open(os.path.join(base_path, path), 'r', encoding='utf-8') as f:\n",
    "            result.append(f.readline())\n",
    "    return result\n",
    "\n",
    "# è¯»å…¥æ•°æ®å¹¶è½¬åŒ–ä¸ºdatasets.Dataset\n",
    "def get_dataset(base_path):\n",
    "\t\t# ä¸ºäº†å±•ç¤ºæ–¹ä¾¿ï¼Œè¿™é‡Œåªå–å‰3ä¸ªæ•°æ®ï¼ŒçœŸå®ä½¿ç”¨éœ€è¦åˆ æ‰åˆ‡ç‰‡æ“ä½œ\n",
    "    pos_data = load_data(os.path.join(base_path, 'pos'))[:3]\n",
    "    neg_data = load_data(os.path.join(base_path, 'neg'))[:3]\n",
    "    \n",
    "\t\t# åˆ—è¡¨åˆå¹¶\n",
    "    texts = pos_data + neg_data\n",
    "\t\t# ç”Ÿæˆæ ‡ç­¾ï¼Œå…¶ä¸­ä½¿ç”¨ '1.' å’Œ '0.' æ˜¯å› ä¸ºéœ€è¦è½¬åŒ–ä¸ºæµ®ç‚¹æ•°ï¼Œè¦ä¸ç„¶æ¨¡å‹è®­ç»ƒæ—¶ä¼šæŠ¥é”™\n",
    "    labels = [[1., 0.]]*len(pos_data) + [[0., 1.]] * len(neg_data)\n",
    "    dataset = Dataset.from_dict({'texts':texts, 'labels':labels})\n",
    "    return dataset\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train_dataset = get_dataset('../aclImdb/train/')\n",
    "test_dataset = get_dataset('../aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['texts', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "{'texts': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset['labels'])\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776e372eef02407588e7fc18ba8e7325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2c3cead9b647fbab121c5469b12674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# è½½å…¥æ–‡æœ¬æ ‡è®°å™¨\n",
    "# cache_diræ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„åœ°å€\n",
    "cache_dir=\"bert-base-uncased1/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "\n",
    "# å°†æ•°æ®è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„æ ¼å¼\n",
    "# è®¾ç½®æœ€å¤§é•¿åº¦\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# ä½¿ç”¨æ–‡æœ¬æ ‡è®°å™¨å¯¹textsè¿›è¡Œç¼–ç \n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "\n",
    "# å°†æ•°æ®ä¿å­˜åˆ°æœ¬åœ°\n",
    "# train_dataset.save_to_disk('./data/train_dataset')\n",
    "# test_dataset.save_to_disk('./data/test_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "# è®¾å®šä½¿ç”¨çš„GPUç¼–å·ï¼Œä¹Ÿå¯ä»¥ä¸è®¾ç½®ï¼Œä½†trainerä¼šé»˜è®¤ä½¿ç”¨å¤šGPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyzhang/.conda/envs/py39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased1/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/xyzhang/.conda/envs/py39/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# å°†num_labelsè®¾ç½®ä¸º2ï¼Œå› ä¸ºæˆ‘ä»¬è®­ç»ƒçš„ä»»åŠ¡ä¸º2åˆ†ç±»\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased1/', num_labels=2)\n",
    "\n",
    "# åŠ è½½å¤„ç†å¥½çš„æ•°æ®\n",
    "train_dataset = Dataset.load_from_disk('./data/train_dataset/')\n",
    "test_dataset = Dataset.load_from_disk('./data/test_dataset/')\n",
    "'''\n",
    "è¿™é‡Œå¯ä»¥å†»ç»“BERTå‚æ•°ï¼Œåªè®­ç»ƒæœ€åä¸€å±‚äºŒåˆ†ç±»å±‚ï¼Œä¸è¿‡æˆ‘è¿™é‡Œé‡‡ç”¨çš„å…¨éƒ¨è®­ç»ƒç­–ç•¥\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "# è®­ç»ƒè¶…å‚é…ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./my_results',          # output directory ç»“æœè¾“å‡ºåœ°å€\n",
    "    num_train_epochs=10,              # total # of training epochs è®­ç»ƒæ€»æ‰¹æ¬¡\n",
    "    per_device_train_batch_size=32,  # batch size per device during training è®­ç»ƒæ‰¹å¤§å°\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation è¯„ä¼°æ‰¹å¤§å°\n",
    "    logging_dir='./my_logs',            # directory for storing logs æ—¥å¿—å­˜å‚¨ä½ç½®\n",
    ")\n",
    "\n",
    "# åˆ›å»ºTrainer\n",
    "trainer = Trainer(\n",
    "    model=model.to('cuda'),              # the instantiated ğŸ¤— Transformers model to be trained éœ€è¦è®­ç»ƒçš„æ¨¡å‹\n",
    "    args=training_args,                  # training arguments, defined above è®­ç»ƒå‚æ•°\n",
    "    train_dataset=train_dataset,         # training dataset è®­ç»ƒé›†\n",
    "    eval_dataset=test_dataset,           # evaluation dataset æµ‹è¯•é›†\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7820/7820 1:27:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1564' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 10:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# å¼€å§‹è¯„ä¼°æ¨¡å‹\n",
    "trainer.evaluate()\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹ ä¼šä¿å­˜åˆ°é…ç½®çš„output_dirå¤„\n",
    "trainer.save_model()\n",
    "torch.save(model.state_dict(), 'model_save.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3857584297657013,\n",
       " 'eval_runtime': 171.263,\n",
       " 'eval_samples_per_second': 145.974,\n",
       " 'eval_steps_per_second': 4.566,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31måœ¨å½“å‰å•å…ƒæ ¼æˆ–ä¸Šä¸€ä¸ªå•å…ƒæ ¼ä¸­æ‰§è¡Œä»£ç æ—¶ Kernel å´©æºƒã€‚\n",
      "\u001b[1;31mè¯·æŸ¥çœ‹å•å…ƒæ ¼ä¸­çš„ä»£ç ï¼Œä»¥ç¡®å®šæ•…éšœçš„å¯èƒ½åŸå› ã€‚\n",
      "\u001b[1;31må•å‡»<a href='https://aka.ms/vscodeJupyterKernelCrash'>æ­¤å¤„</a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\u001b[1;31mæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ Jupyter <a href='command:jupyter.viewOutput'>log</a>ã€‚"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è¯„ä¼°æ¨¡å‹\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 æ¨¡å‹å‡†ç¡®ç‡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Set the device to use the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# è®¾å®šä½¿ç”¨çš„GPUç¼–å·ï¼Œä¹Ÿå¯ä»¥ä¸è®¾ç½®ï¼Œä½†trainerä¼šé»˜è®¤ä½¿ç”¨å¤šGPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "output_config_file = './my_results/config.json'\n",
    "output_model_file = 'model_save.bin'\n",
    "\n",
    "config = BertConfig.from_json_file(output_config_file)\n",
    "model = BertForSequenceClassification(config).to(device)\n",
    "state_dict = torch.load(output_model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "test_dataset = Dataset.load_from_disk('./data/test_dataset/')\n",
    "cache_dir=\"bert-base-uncased1/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "\n",
    "# é™ä½æ‰¹å¤„ç†å¤§å°å’Œåºåˆ—é•¿åº¦\n",
    "batch_size = 16\n",
    "\n",
    "data = test_dataset['texts']\n",
    "data = tokenizer(data, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "# ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡è¿›è¡Œæ¨ç†\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(0, len(data['input_ids']), batch_size):\n",
    "    batch_data = {k: v[i:i+batch_size].to(device) for k, v in data.items()}\n",
    "    with torch.no_grad():\n",
    "        batch_preds = model(**batch_data).logits\n",
    "    preds.extend(np.argmax(batch_preds.detach().cpu().numpy(), axis=-1))\n",
    "    labels.extend(np.argmax(test_dataset['labels'][i:i+batch_size], axis=-1))\n",
    "\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# è®¡ç®—å‡†ç¡®ç‡\n",
    "accuracy = sum(preds == labels) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†ä¸€ç»´æ•°ç»„è½¬æ¢ä¸ºCSVæ–‡ä»¶ä¸­çš„å•åˆ—\n",
    "with open('output1.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(preds)  # å†™å…¥å¤´éƒ¨\n",
    "\n",
    "# å°†ä¸€ç»´æ•°ç»„è½¬æ¢ä¸ºCSVæ–‡ä»¶ä¸­çš„å•åˆ—\n",
    "with open('output2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)  # å†™å…¥å¤´éƒ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# è¯»å– CSV æ–‡ä»¶\n",
    "with open('output1.csv', 'r') as csvfile1:\n",
    "    reader1 = csv.reader(csvfile1)\n",
    "    rows1 = list(reader1)\n",
    "\n",
    "rows1 = np.array(rows1).astype(np.int32)\n",
    "preds = rows1[0]\n",
    "\n",
    "with open('output2.csv', 'r') as csvfile2:\n",
    "    reader2 = csv.reader(csvfile2)\n",
    "    rows2 = list(reader2)\n",
    "\n",
    "rows2 = np.array(rows2).astype(np.int32)\n",
    "labels = rows2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—å‡†ç¡®ç‡\n",
    "accuracy = sum(preds == labels) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è°ƒè¯•ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(test_dataset[:3]['labels'],axis=-1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dir=\"bert-base-uncased/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "data = test_dataset[:3]['texts']\n",
    "data = tokenizer(data, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "preds = model(**data)\n",
    "preds = np.argmax(preds.logits.detach().numpy(),axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.9377, -5.8652],\n",
      "        [-7.1596,  7.5822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "cache_dir=\"bert-base-uncased/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "data = tokenizer(['This is a good movie', 'This is a bad movie'], max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "print(model(**data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
