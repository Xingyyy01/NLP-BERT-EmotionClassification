{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入原始数据\n",
    "def load_data(base_path):\n",
    "    paths = os.listdir(base_path)\n",
    "    result = []\n",
    "    for path in paths:\n",
    "        with open(os.path.join(base_path, path), 'r', encoding='utf-8') as f:\n",
    "            result.append(f.readline())\n",
    "    return result\n",
    "\n",
    "# 读入数据并转化为datasets.Dataset\n",
    "def get_dataset(base_path):\n",
    "\t\t# 为了展示方便，这里只取前3个数据，真实使用需要删掉切片操作\n",
    "    pos_data = load_data(os.path.join(base_path, 'pos'))[:3]\n",
    "    neg_data = load_data(os.path.join(base_path, 'neg'))[:3]\n",
    "    \n",
    "\t\t# 列表合并\n",
    "    texts = pos_data + neg_data\n",
    "\t\t# 生成标签，其中使用 '1.' 和 '0.' 是因为需要转化为浮点数，要不然模型训练时会报错\n",
    "    labels = [[1., 0.]]*len(pos_data) + [[0., 1.]] * len(neg_data)\n",
    "    dataset = Dataset.from_dict({'texts':texts, 'labels':labels})\n",
    "    return dataset\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = get_dataset('../aclImdb/train/')\n",
    "test_dataset = get_dataset('../aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['texts', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n",
      "{'texts': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_dataset['labels'])\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776e372eef02407588e7fc18ba8e7325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2c3cead9b647fbab121c5469b12674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 载入文本标记器\n",
    "# cache_dir是预训练模型的地址\n",
    "cache_dir=\"bert-base-uncased1/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "\n",
    "# 将数据转化为模型可以接受的格式\n",
    "# 设置最大长度\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# 使用文本标记器对texts进行编码\n",
    "train_dataset = train_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "test_dataset = test_dataset.map(lambda e: tokenizer(e['texts'], truncation=True, padding='max_length', max_length=MAX_LENGTH), batched=True)\n",
    "\n",
    "# 将数据保存到本地\n",
    "# train_dataset.save_to_disk('./data/train_dataset')\n",
    "# test_dataset.save_to_disk('./data/test_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': Value(dtype='string', id=None), 'labels': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "# 设定使用的GPU编号，也可以不设置，但trainer会默认使用多GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyzhang/.conda/envs/py39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased1/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/xyzhang/.conda/envs/py39/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 将num_labels设置为2，因为我们训练的任务为2分类\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased1/', num_labels=2)\n",
    "\n",
    "# 加载处理好的数据\n",
    "train_dataset = Dataset.load_from_disk('./data/train_dataset/')\n",
    "test_dataset = Dataset.load_from_disk('./data/test_dataset/')\n",
    "'''\n",
    "这里可以冻结BERT参数，只训练最后一层二分类层，不过我这里采用的全部训练策略\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "'''\n",
    "# 训练超参配置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./my_results',          # output directory 结果输出地址\n",
    "    num_train_epochs=10,              # total # of training epochs 训练总批次\n",
    "    per_device_train_batch_size=32,  # batch size per device during training 训练批大小\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation 评估批大小\n",
    "    logging_dir='./my_logs',            # directory for storing logs 日志存储位置\n",
    ")\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to('cuda'),              # the instantiated 🤗 Transformers model to be trained 需要训练的模型\n",
    "    args=training_args,                  # training arguments, defined above 训练参数\n",
    "    train_dataset=train_dataset,         # training dataset 训练集\n",
    "    eval_dataset=test_dataset,           # evaluation dataset 测试集\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7820/7820 1:27:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.083700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1564' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 10:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 开始评估模型\n",
    "trainer.evaluate()\n",
    "\n",
    "# 保存模型 会保存到配置的output_dir处\n",
    "trainer.save_model()\n",
    "torch.save(model.state_dict(), 'model_save.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3857584297657013,\n",
       " 'eval_runtime': 171.263,\n",
       " 'eval_samples_per_second': 145.974,\n",
       " 'eval_steps_per_second': 4.566,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 开始评估模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 模型准确率评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, BertConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Set the device to use the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 设定使用的GPU编号，也可以不设置，但trainer会默认使用多GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 加载模型\n",
    "output_config_file = './my_results/config.json'\n",
    "output_model_file = 'model_save.bin'\n",
    "\n",
    "config = BertConfig.from_json_file(output_config_file)\n",
    "model = BertForSequenceClassification(config).to(device)\n",
    "state_dict = torch.load(output_model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 加载数据\n",
    "test_dataset = Dataset.load_from_disk('./data/test_dataset/')\n",
    "cache_dir=\"bert-base-uncased1/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "\n",
    "# 降低批处理大小和序列长度\n",
    "batch_size = 16\n",
    "\n",
    "data = test_dataset['texts']\n",
    "data = tokenizer(data, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "# 使用较小批次进行推理\n",
    "preds = []\n",
    "labels = []\n",
    "for i in range(0, len(data['input_ids']), batch_size):\n",
    "    batch_data = {k: v[i:i+batch_size].to(device) for k, v in data.items()}\n",
    "    with torch.no_grad():\n",
    "        batch_preds = model(**batch_data).logits\n",
    "    preds.extend(np.argmax(batch_preds.detach().cpu().numpy(), axis=-1))\n",
    "    labels.extend(np.argmax(test_dataset['labels'][i:i+batch_size], axis=-1))\n",
    "\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = sum(preds == labels) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一维数组转换为CSV文件中的单列\n",
    "with open('output1.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(preds)  # 写入头部\n",
    "\n",
    "# 将一维数组转换为CSV文件中的单列\n",
    "with open('output2.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)  # 写入头部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "with open('output1.csv', 'r') as csvfile1:\n",
    "    reader1 = csv.reader(csvfile1)\n",
    "    rows1 = list(reader1)\n",
    "\n",
    "rows1 = np.array(rows1).astype(np.int32)\n",
    "preds = rows1[0]\n",
    "\n",
    "with open('output2.csv', 'r') as csvfile2:\n",
    "    reader2 = csv.reader(csvfile2)\n",
    "    rows2 = list(reader2)\n",
    "\n",
    "rows2 = np.array(rows2).astype(np.int32)\n",
    "labels = rows2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "# 计算准确率\n",
    "accuracy = sum(preds == labels) / len(labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.argmax(test_dataset[:3]['labels'],axis=-1)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dir=\"bert-base-uncased/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "data = test_dataset[:3]['texts']\n",
    "data = tokenizer(data, max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "preds = model(**data)\n",
    "preds = np.argmax(preds.logits.detach().numpy(),axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.9377, -5.8652],\n",
      "        [-7.1596,  7.5822]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "cache_dir=\"bert-base-uncased/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(cache_dir)\n",
    "data = tokenizer(['This is a good movie', 'This is a bad movie'], max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "print(model(**data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
